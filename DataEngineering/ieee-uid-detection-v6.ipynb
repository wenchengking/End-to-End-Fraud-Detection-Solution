{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This notebook is the direct descendof the original notebook ieee-uid-detection.ipynb. The only difference is that this notebook generates the full list of uid based on train datset."]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: pyarrow in ./.local/lib/python3.10/site-packages (11.0.0)\n","Requirement already satisfied: numpy>=1.16.6 in ./.local/lib/python3.10/site-packages (from pyarrow) (1.23.4)\n","Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: fastparquet in ./.local/lib/python3.10/site-packages (2023.2.0)\n","Requirement already satisfied: fsspec in ./.local/lib/python3.10/site-packages (from fastparquet) (2023.1.0)\n","Requirement already satisfied: pandas>=1.5.0 in ./.local/lib/python3.10/site-packages (from fastparquet) (1.5.1)\n","Requirement already satisfied: numpy>=1.20.3 in ./.local/lib/python3.10/site-packages (from fastparquet) (1.23.4)\n","Requirement already satisfied: packaging in ./.local/lib/python3.10/site-packages (from fastparquet) (21.3)\n","Requirement already satisfied: cramjam>=2.3 in ./.local/lib/python3.10/site-packages (from fastparquet) (2.6.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.5.0->fastparquet) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in ./.local/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging->fastparquet) (2.4.7)\n","Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas>=1.5.0->fastparquet) (1.16.0)\n"]}],"source":["!pip install pyarrow\n","!pip install fastparquet"]},{"cell_type":"code","execution_count":41,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# General imports\n","import numpy as np\n","import pandas as pd\n","import os, sys, gc, warnings, random, datetime, math, psutil\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from multiprocessing import Pool\n","\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":42,"metadata":{"trusted":true},"outputs":[],"source":["########################### Helpers\n","#################################################################################\n","## Multiprocessing Run.\n","# :df - DataFrame to split                      # type: pandas DataFrame\n","# :func - Function to apply on each split       # type: python function\n","# This function is NOT 'bulletproof', be carefull and pass only correct types of variables.\n","def df_parallelize_run(df, func):\n","    num_partitions, num_cores = psutil.cpu_count(), psutil.cpu_count()  # number of partitions and cores\n","    df_split = np.array_split(df, num_partitions)\n","    pool = Pool(num_cores)\n","    df = pd.concat(pool.map(func, df_split))\n","    pool.close()\n","    pool.join()\n","    return df\n","\n","def check_state():\n","    if LOCAL_TEST:\n","        bad_uids = full_df.groupby(['uid'])['isFraud'].agg(['nunique', 'count'])\n","        bad_uids = bad_uids[(bad_uids['nunique']==2)]\n","        print('Inconsistent groups',len(bad_uids))\n","\n","    print('Cleaning done...')\n","    print('Total groups:', len(full_df['uid'].unique()), \n","          '| Total items:', len(full_df),\n","          '| Total fraud', full_df['isFraud'].sum())\n","    \n","    \n","########################### Sainity check \n","def sanity_check_run(temp_df, verbose=False):\n","    temp_df = temp_df.copy()\n","    temp_df = temp_df.sort_values(by='TransactionID').reset_index(drop=True)\n","    bad_uids_groups = pd.DataFrame()\n","\n","    \"\"\"\n","    for col in ['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']:\n","        temp_df['sanity_check'] = temp_df.groupby(['uid'])[col].shift()\n","        temp_df['sanity_check'] = (temp_df[col]-temp_df['sanity_check']).fillna(0).clip(None,0)\n","\n","        bad_uids = temp_df.groupby(['uid'])['sanity_check'].agg(['sum']).reset_index()\n","        bad_uids = bad_uids[bad_uids['sum']<0]\n","        bad_uids_groups = pd.concat([bad_uids_groups,bad_uids])\n","        if verbose: print(col, len(bad_uids), bad_uids['uid'].values[:2])\n","    \"\"\"\n","    \n","    bad_uids = temp_df.groupby(['uid'])['V313'].agg(['nunique']).reset_index()\n","    bad_uids = bad_uids[(bad_uids['nunique']>2)]\n","    bad_uids_groups = pd.concat([bad_uids_groups,bad_uids])\n","    if verbose: print('V313:', len(bad_uids), bad_uids['uid'].values[:2])\n","\n","    bad_uids_groups = bad_uids_groups[['uid']].drop_duplicates()\n","    if verbose: print('Total bad groups:', len(bad_uids_groups))\n","    return bad_uids_groups\n","\n","\n","\n","def parallel_check(bad_uids_groups):\n","    bad_uids_items = []\n","    if True:\n","        for cur_uid in list(bad_uids_groups['uid'].unique()):\n","            temp_df = full_df[full_df['uid']==cur_uid].reset_index(drop=True)\n","            v313_values = temp_df['V313'].value_counts()\n","            if len(v313_values)>1: \n","                v313_values = [[col for col in list(v313_values.index)[:2] if col!=0][0]] + [0]\n","            else: \n","                v313_values = [list(v313_values.index)[0]]+[0]\n","                \n","            for i in range(1,len(temp_df)):\n","                item_1 = temp_df.iloc[i]\n","                item_2 = temp_df.iloc[i-1]\n","\n","                check_if_match = temp_df.drop([i])\n","                check_if_match = sanity_check_run(check_if_match)\n","                if len(check_if_match) == 0:\n","                    bad_uids_items.append(item_1['TransactionID'])\n","                    break\n","\n","                if i!=1:\n","                    check_if_match = temp_df.drop([i-1])\n","                    check_if_match = sanity_check_run(check_if_match)\n","                    if len(check_if_match) == 0:\n","                        bad_uids_items.append(item_2['TransactionID'])\n","                        break\n","                \n","                \"\"\"\n","                for col in ['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']:\n","                    check_sanity = item_1[col]<item_2[col]\n","                    if check_sanity:\n","                        bad_uids_items.append(item_1['TransactionID'])\n","                        break \n","                        \n","                if check_sanity:\n","                    break\n","                \"\"\"    \n","                check_sanity = item_1['V313'] not in v313_values\n","                if check_sanity:\n","                    bad_uids_items.append(item_1['TransactionID'])\n","                    break \n","                    \n","                if temp_df['TransactionID'].isin(problem_items['TransactionID']).sum()==0:\n","                    if cur_uid>=0:\n","                        check_sanity = ((item_2['DT_day']-item_1['uid_td_D3'])**2)**0.5>1\n","\n","                        if check_sanity:\n","                            bad_uids_items.append(item_1['TransactionID'])\n","                            break\n","\n","    bad_uids_items = pd.DataFrame(bad_uids_items, columns=['uid'])\n","    return bad_uids_items\n"," "]},{"cell_type":"code","execution_count":43,"metadata":{"trusted":true},"outputs":[],"source":["LOCAL_TEST = True\n","CHECK_ORDER = True\n","TRUST_D1 = True\n","MULTI_UID_CHECK = False\n","FULL_GROUP_CHECK = False"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We updated the part below so that: \n","- we can use only the train dataset\n","- we can update `LOCAL_TEST` to False so that we can generate uid for the entire set"]},{"cell_type":"code","execution_count":44,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Load Data\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TransactionID</th>\n","      <th>isFraud</th>\n","      <th>TransactionDT</th>\n","      <th>TransactionAmt</th>\n","      <th>ProductCD</th>\n","      <th>card1</th>\n","      <th>card2</th>\n","      <th>card3</th>\n","      <th>card4</th>\n","      <th>card5</th>\n","      <th>...</th>\n","      <th>uid_td_D5</th>\n","      <th>uid_td_D6</th>\n","      <th>uid_td_D7</th>\n","      <th>uid_td_D8</th>\n","      <th>uid_td_D10</th>\n","      <th>uid_td_D11</th>\n","      <th>uid_td_D12</th>\n","      <th>uid_td_D13</th>\n","      <th>uid_td_D14</th>\n","      <th>uid_td_D15</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2987000</td>\n","      <td>0</td>\n","      <td>86400</td>\n","      <td>68.5</td>\n","      <td>800657</td>\n","      <td>13926</td>\n","      <td>NaN</td>\n","      <td>150.0</td>\n","      <td>9524.0</td>\n","      <td>142.0</td>\n","      <td>...</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>2017-11-18</td>\n","      <td>2017-11-18</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>2017-12-01</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2987001</td>\n","      <td>0</td>\n","      <td>86401</td>\n","      <td>29.0</td>\n","      <td>800657</td>\n","      <td>2755</td>\n","      <td>404.0</td>\n","      <td>150.0</td>\n","      <td>347386.0</td>\n","      <td>102.0</td>\n","      <td>...</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>2017-12-01</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>2017-12-01</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2987002</td>\n","      <td>0</td>\n","      <td>86469</td>\n","      <td>59.0</td>\n","      <td>800657</td>\n","      <td>4663</td>\n","      <td>490.0</td>\n","      <td>150.0</td>\n","      <td>719649.0</td>\n","      <td>166.0</td>\n","      <td>...</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>2017-12-01</td>\n","      <td>2017-01-20</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>2017-01-20</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2987003</td>\n","      <td>0</td>\n","      <td>86499</td>\n","      <td>50.0</td>\n","      <td>800657</td>\n","      <td>18132</td>\n","      <td>567.0</td>\n","      <td>150.0</td>\n","      <td>347386.0</td>\n","      <td>117.0</td>\n","      <td>...</td>\n","      <td>2017-12-01</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>2017-09-08</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>2017-08-12</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2987004</td>\n","      <td>0</td>\n","      <td>86506</td>\n","      <td>50.0</td>\n","      <td>62397</td>\n","      <td>4497</td>\n","      <td>514.0</td>\n","      <td>150.0</td>\n","      <td>347386.0</td>\n","      <td>102.0</td>\n","      <td>...</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 421 columns</p>\n","</div>"],"text/plain":["   TransactionID  isFraud  TransactionDT  TransactionAmt  ProductCD  card1  \\\n","0        2987000        0          86400            68.5     800657  13926   \n","1        2987001        0          86401            29.0     800657   2755   \n","2        2987002        0          86469            59.0     800657   4663   \n","3        2987003        0          86499            50.0     800657  18132   \n","4        2987004        0          86506            50.0      62397   4497   \n","\n","   card2  card3     card4  card5  ...   uid_td_D5  uid_td_D6  uid_td_D7  \\\n","0    NaN  150.0    9524.0  142.0  ...        None       None       None   \n","1  404.0  150.0  347386.0  102.0  ...        None       None       None   \n","2  490.0  150.0  719649.0  166.0  ...        None       None       None   \n","3  567.0  150.0  347386.0  117.0  ...  2017-12-01       None       None   \n","4  514.0  150.0  347386.0  102.0  ...        None       None       None   \n","\n","   uid_td_D8  uid_td_D10  uid_td_D11 uid_td_D12  uid_td_D13  uid_td_D14  \\\n","0       None  2017-11-18  2017-11-18       None        None        None   \n","1       None  2017-12-01        None       None        None        None   \n","2       None  2017-12-01  2017-01-20       None        None        None   \n","3       None  2017-09-08        None       None        None        None   \n","4       None        None        None       None        None        None   \n","\n","   uid_td_D15  \n","0  2017-12-01  \n","1  2017-12-01  \n","2  2017-01-20  \n","3  2017-08-12  \n","4        None  \n","\n","[5 rows x 421 columns]"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["########################### DATA LOAD\n","#################################################################################\n","print('Load Data')\n","url = r'https://drive.google.com/file/d/1EM2i0wSQYeEa1B0gE0P2DFrJGSFCsKwm/view?usp=share_link'\n","url = 'https://drive.google.com/uc?id=' + url.split('/')[-2]\n","full_df = pd.read_parquet(url)\n","full_df.head()"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["# commented to get the full test\n","# if LOCAL_TEST:\n","#     full_df = full_df.iloc[:10000] #full_df[(full_df['DT_M']==12)]``"]},{"cell_type":"code","execution_count":46,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of transactions: 589236\n"]}],"source":["########################### Base prepartion\n","\n","full_df['full_addr'] = full_df['addr1'].astype(str)+'_'+full_df['addr2'].astype(str)\n","\n","for col in ['D'+str(i) for i in [1,2,3,5,10,11,15]]: \n","    new_col = 'uid_td_'+str(col)\n","    \n","    full_df[new_col] = full_df['TransactionDT'] / (24*60*60)\n","    full_df[new_col] = np.floor(full_df[new_col] - full_df[col]) + 1000\n","\n","full_df['DT_day'] = np.floor(full_df['TransactionDT']/(24*60*60)) + 1000\n","\n","full_df['TransactionAmt_fix'] = np.round(full_df['TransactionAmt'],2)\n","full_df['V313_fix'] = np.round(full_df['V313'],2)\n","full_df['uid'] = np.nan\n","\n","v_cols = []\n","v_fix_cols = []\n","for col in ['V'+str(i) for i in range(1,340)]:\n","    if (full_df[col].fillna(0) - full_df[col].fillna(0).astype(int)).sum()!=0:\n","        if col not in ['V313']:\n","            v_cols.append(col)\n","            v_fix_cols.append(col+'_fix')\n","            full_df[col+'_fix_ground'] = np.round(full_df[col],2)\n","            full_df[col+'_fix'] = full_df[col+'_fix_ground'] + full_df['TransactionAmt_fix']\n","\n","global_bad_items = full_df[full_df['D1'].isna()]\n","full_df = full_df[~full_df['TransactionDT'].isin(global_bad_items['TransactionDT'])]\n","all_items = full_df.copy()\n","bkp_items = full_df.copy()\n","\n","print('Total number of transactions:', len(full_df))"]},{"cell_type":"code","execution_count":47,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Single transaction 70847\n"]}],"source":["########################### Single Transaction\n","# Let's filter single card apearence card1/D1 -> single transaction per card\n","full_df['count'] = full_df.groupby(['card1','uid_td_D1'])['TransactionID'].transform('count')\n","single_items = full_df[full_df['count']==1]\n","single_items['uid'] = single_items['TransactionID']\n","del full_df, single_items['count']\n","\n","all_items = all_items[~all_items['TransactionID'].isin(single_items['TransactionID'])]\n","print('Single transaction',len(single_items))"]},{"cell_type":"code","execution_count":48,"metadata":{"trusted":true},"outputs":[],"source":["### Clean full_df\n","full_df = pd.DataFrame()\n","###"]},{"cell_type":"code","execution_count":49,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["First time in dataset 78680\n"]}],"source":["# First appearance of card1\n","\n","first_df = all_items.copy()\n","first_df['counts'] = first_df.groupby(['card1','uid_td_D1']).cumcount()\n","first_df = first_df[first_df['counts']==0]\n","del first_df['counts']\n","\n","first_df['uid'] = first_df['TransactionID']\n","print('First time in dataset', len(first_df))\n","\n","full_df = pd.concat([full_df,first_df])\n","full_df = full_df.sort_values(by='TransactionID').reset_index(drop=True)\n","del first_df"]},{"cell_type":"code","execution_count":50,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Inconsistent groups 0\n","Cleaning done...\n","Total groups: 78680 | Total items: 78680 | Total fraud 2563\n"]}],"source":["check_state()"]},{"cell_type":"code","execution_count":51,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Roots out of dataset 156543\n"]}],"source":["# Lets Check unassigned items again\n","# Let's find itmes with roots out of our dataset\n","nan_df_check = all_items[~all_items['TransactionID'].isin(full_df['TransactionID'])]\n","\n","# if 'uid_td_D3'>1000 it means that root item is in our dataset\n","# >1000 will also filter NaNs values\n","nan_df_check['uid'] = np.where(nan_df_check['uid_td_D3']>=1001, \n","                               np.nan, nan_df_check['TransactionID'])\n","nan_df_check = nan_df_check[~nan_df_check['uid'].isna()]\n","\n","full_df = pd.concat([full_df,nan_df_check])\n","full_df = full_df.sort_values(by='TransactionID').reset_index(drop=True)\n","\n","print('Roots out of dataset', len(nan_df_check))\n","#del nan_df_check\n","out_of_bonds = nan_df_check[['TransactionID']]"]},{"cell_type":"code","execution_count":52,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Inconsistent groups 0\n","Cleaning done...\n","Total groups: 235223 | Total items: 235223 | Total fraud 9686\n"]}],"source":["check_state()"]},{"cell_type":"code","execution_count":53,"metadata":{"trusted":true},"outputs":[],"source":["########################### VERY IMPORTANT\n","# Do not do sanity D3 check for gap items\n","problem_items = full_df[(full_df['uid_td_D3']>1182)&(full_df['uid_td_D3']<1213)]\n","\n","out_of_bonds = pd.concat([out_of_bonds, problem_items[['TransactionID']]])"]},{"cell_type":"code","execution_count":54,"metadata":{"trusted":true},"outputs":[],"source":["########################### Sort\n","all_items = all_items.sort_values(by='TransactionID').reset_index(drop=True)\n","single_items = single_items.sort_values(by='TransactionID').reset_index(drop=True)\n","full_df = full_df.sort_values(by='TransactionID').reset_index(drop=True)\n","out_of_bonds = out_of_bonds.sort_values(by='TransactionID').reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{},"source":["We found all root items. There is just very rare cases that a new root can appear \n","_____"]},{"cell_type":"code","execution_count":55,"metadata":{"trusted":true},"outputs":[],"source":["def find_and_append_root(df):\n","    new_uids_items = {'TransactionID': [],\n","                      'uid': [],\n","                      }\n","\n","    for i in range(len(df)):\n","        item = df.iloc[i]\n","        if item['TransactionID'] not in list(problem_items['TransactionID']):\n","            mask_1 = bkp_items['card1'] == item['card1']\n","            mask_2 = bkp_items['uid_td_D1'] == item['uid_td_D1']\n","            mask_3 = bkp_items['TransactionID'] < item['TransactionID']\n","            mask_4 = ((bkp_items['DT_day'] == item['uid_td_D3'] + 1)|\n","                      (bkp_items['DT_day'] == item['uid_td_D3'] - 1)|\n","                      (bkp_items['DT_day'] == item['uid_td_D3']))\n","\n","            df_masked = bkp_items[mask_1 & mask_2 & mask_3 & mask_4]\n","            no_match = len(df_masked) == 0\n","\n","            if no_match:\n","                new_uids_items['TransactionID'].append(item['TransactionID'])\n","                new_uids_items['uid'].append(item['TransactionID'])\n"," \n","    return_df = pd.DataFrame.from_dict(new_uids_items)\n","    return return_df"]},{"cell_type":"code","execution_count":56,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Items to check: 283166\n","Assigned root items: 321\n","Inconsistent groups 0\n","Cleaning done...\n","Total groups: 235544 | Total items: 235544 | Total fraud 9690\n"]}],"source":["########################### PART X - > 100% Root\n","\n","nan_df = all_items[~all_items['TransactionID'].isin(full_df['TransactionID'])]\n","nan_df = nan_df[nan_df['DT_M']<18]\n","print('Items to check:', len(nan_df))\n","\n","df_cleaned = df_parallelize_run(nan_df, find_and_append_root)\n","df_cleaned = df_cleaned[~df_cleaned['uid'].isna()]\n","\n","df_cleaned.index = df_cleaned['TransactionID']\n","temp_dict = df_cleaned['uid'].to_dict()\n","nan_df['uid'] = nan_df['TransactionID'].map(temp_dict)\n","nan_df = nan_df[~nan_df['uid'].isna()]\n","print('Assigned root items:', len(nan_df))\n","\n","# append found items\n","full_df = pd.concat([full_df,nan_df]).sort_values(by='TransactionID').reset_index(drop=True)\n","check_state()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":57,"metadata":{"trusted":true},"outputs":[],"source":["def append_item_to_uid(df):\n","    new_uids_items = {'TransactionID': [],\n","                      'uid': [],\n","                      }\n","\n","    for i in range(len(df)):\n","        item = df.iloc[i]\n","\n","        mask_1 = full_df['card1'] == item['card1']\n","        mask_2 = full_df['uid_td_D1'] == item['uid_td_D1']\n","        mask_3 = full_df['TransactionID'] < item['TransactionID']\n","        mask_4 = full_df['DT_day'] <= item['uid_td_D3'] + 1  # +1 just to ensure that there is no\n","        df_masked = full_df[mask_1 & mask_2 & mask_3 & mask_4]\n","\n","        has_match = len(df_masked) > 0\n","        can_be_root = True\n","\n","        # New check\n","        # addr2 should be nan or same as group addr2\n","        for col in ['addr2','addr1']:\n","            if has_match:\n","                if not np.isnan(item[col]):\n","                    mask = ((df_masked[col] == item[col]) | (df_masked[col].isna()))\n","                    df_masked = df_masked[mask]\n","                else:\n","                    df_masked = df_masked\n","\n","                if len(df_masked) == 0:\n","                    has_match = False\n","        \"\"\"\n","        # Order                   \n","        for col in ['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']:\n","            if has_match:\n","                mask = df_masked[col] <= item[col]\n","                df_masked = df_masked[mask]\n","\n","                if len(df_masked) == 0:\n","                    has_match = False\n","        \"\"\"\n","        \n","        if has_match:\n","            mask = (df_masked['TransactionID'] > item['TransactionID']).astype(int)\n","            for col in v_cols:\n","                mask += (df_masked[col+'_fix'] == item[col+'_fix_ground']).astype(int)\n","            mask = mask>0\n","            df_masked = df_masked[mask]\n","            \n","            if len(df_masked) == 0:\n","                has_match = False\n","                    \n","        # Assign  \n","        if has_match and len(df_masked['uid'].unique()) == 1:\n","            check_if_match = df_masked.append(item)\n","            check_if_match = sanity_check_run(check_if_match)\n","            if len(check_if_match) == 0:\n","                new_uids_items['TransactionID'].append(item['TransactionID'])\n","                new_uids_items['uid'].append(df_masked['uid'].unique()[0])\n","\n","\n","    return_df = pd.DataFrame.from_dict(new_uids_items)\n","    return return_df"]},{"cell_type":"code","execution_count":58,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Check round: 0\n","Items to check: 282845\n"]}],"source":["########################### PART X - > 100% single match\n","for i in range(5):\n","    print('Check round:', i)\n","        \n","    nan_df = all_items[~all_items['TransactionID'].isin(full_df['TransactionID'])]\n","    print('Items to check:', len(nan_df))\n","\n","    df_cleaned = df_parallelize_run(nan_df, append_item_to_uid)\n","    df_cleaned = df_cleaned[~df_cleaned['uid'].isna()]\n","\n","    df_cleaned.index = df_cleaned['TransactionID']\n","    temp_dict = df_cleaned['uid'].to_dict()\n","    nan_df['uid'] = nan_df['TransactionID'].map(temp_dict)\n","    nan_df = nan_df[~nan_df['uid'].isna()]\n","    print('Assigned items:', len(nan_df))\n","\n","    # append found items\n","    full_df = pd.concat([full_df,nan_df]).sort_values(by='TransactionID').reset_index(drop=True)\n","    \n","    for i in range(100):\n","        bad_uids_groups = sanity_check_run(full_df, False)\n","        if len(bad_uids_groups)==0:\n","            break\n","        elif len(bad_uids_groups)>64:\n","            bad_uids_items = df_parallelize_run(bad_uids_groups, parallel_check)\n","        else:\n","            bad_uids_items = parallel_check(bad_uids_groups)\n","\n","        print('Found bad items', len(bad_uids_items))\n","        full_df['uid'] = np.where(full_df['TransactionID'].isin(bad_uids_items['uid']), np.nan, full_df['uid'])\n","        full_df = full_df[~full_df['uid'].isna()].sort_values(by='TransactionID').reset_index(drop=True)\n","        if len(bad_uids_items)<2:\n","            break\n","    check_state()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def find_and_append_root_test(df):\n","    new_uids_items = {'TransactionID': [],\n","                      'uid': [],\n","                      }\n","\n","    for i in range(len(df)):\n","        item = df.iloc[i]\n","        if item['TransactionID'] not in list(problem_items['TransactionID']):\n","            mask_1 = bkp_items['card1'] == item['card1']\n","            mask_2 = bkp_items['uid_td_D1'] == item['uid_td_D1']\n","            mask_3 = bkp_items['TransactionID'] < item['TransactionID']\n","            mask_4 = ((bkp_items['DT_day'] == item['uid_td_D3'] + 1)|\n","                      (bkp_items['DT_day'] == item['uid_td_D3'] - 1)|\n","                      (bkp_items['DT_day'] == item['uid_td_D3']))\n","\n","            df_masked = bkp_items[mask_1 & mask_2 & mask_3 & mask_4]\n","            no_match = len(df_masked) == 0\n","\n","            if no_match:\n","                new_uids_items['TransactionID'].append(item['TransactionID'])\n","                new_uids_items['uid'].append(item['TransactionID'])\n"," \n","    return_df = pd.DataFrame.from_dict(new_uids_items)\n","    return return_df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Items to check: 0\n","Assigned root items: 0\n","Inconsistent groups 0\n","Cleaning done...\n","Total groups: 4173 | Total items: 5602 | Total fraud 206\n"]}],"source":["########################### PART X - > 100% Root\n","\n","nan_df = all_items[~all_items['TransactionID'].isin(full_df['TransactionID'])]\n","nan_df = nan_df[nan_df['DT_M']>18]\n","print('Items to check:', len(nan_df))\n","\n","df_cleaned = df_parallelize_run(nan_df, find_and_append_root_test)\n","df_cleaned = df_cleaned[~df_cleaned['uid'].isna()]\n","df_cleaned = df_cleaned[~df_cleaned['TransactionID'].isin(full_df['TransactionID'])]\n","\n","df_cleaned.index = df_cleaned['TransactionID']\n","temp_dict = df_cleaned['uid'].to_dict()\n","nan_df['uid'] = nan_df['TransactionID'].map(temp_dict)\n","nan_df = nan_df[~nan_df['uid'].isna()]\n","print('Assigned root items:', len(nan_df))\n","\n","# append found items\n","full_df = pd.concat([full_df,nan_df]).sort_values(by='TransactionID').reset_index(drop=True)\n","check_state()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Check round: 0\n","Items to check: 102\n","Assigned items: 21\n","Found bad items 12\n","Found bad items 2\n","Inconsistent groups 0\n","Cleaning done...\n","Total groups: 4173 | Total items: 5609 | Total fraud 206\n","Check round: 1\n","Items to check: 95\n","Assigned items: 21\n","Found bad items 12\n","Found bad items 2\n","Inconsistent groups 0\n","Cleaning done...\n","Total groups: 4173 | Total items: 5616 | Total fraud 206\n","Check round: 2\n","Items to check: 88\n","Assigned items: 16\n","Found bad items 12\n","Found bad items 2\n","Inconsistent groups 0\n","Cleaning done...\n","Total groups: 4173 | Total items: 5618 | Total fraud 206\n"]}],"source":["########################### PART X - > 100% single match\n","for i in range(3):\n","    print('Check round:', i)\n","        \n","    nan_df = all_items[~all_items['TransactionID'].isin(full_df['TransactionID'])]\n","    print('Items to check:', len(nan_df))\n","\n","    df_cleaned = df_parallelize_run(nan_df, append_item_to_uid)\n","    df_cleaned = df_cleaned[~df_cleaned['uid'].isna()]\n","\n","    df_cleaned.index = df_cleaned['TransactionID']\n","    temp_dict = df_cleaned['uid'].to_dict()\n","    nan_df['uid'] = nan_df['TransactionID'].map(temp_dict)\n","    nan_df = nan_df[~nan_df['uid'].isna()]\n","    print('Assigned items:', len(nan_df))\n","\n","    # append found items\n","    full_df = pd.concat([full_df,nan_df]).sort_values(by='TransactionID').reset_index(drop=True)\n","    \n","    for i in range(100):\n","        bad_uids_groups = sanity_check_run(full_df, False)\n","        if len(bad_uids_groups)==0:\n","            break\n","        elif len(bad_uids_groups)>64:\n","            bad_uids_items = df_parallelize_run(bad_uids_groups, parallel_check)\n","        else:\n","            bad_uids_items = parallel_check(bad_uids_groups)\n","\n","        print('Found bad items', len(bad_uids_items))\n","        full_df['uid'] = np.where(full_df['TransactionID'].isin(bad_uids_items['uid']), np.nan, full_df['uid'])\n","        full_df = full_df[~full_df['uid'].isna()].sort_values(by='TransactionID').reset_index(drop=True)\n","        if len(bad_uids_items)<2:\n","            break\n","    check_state()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["full_df[['TransactionID','uid']].to_csv('uids_part_1_v6.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def find_multigroup(df):\n","    new_uids_items = {'TransactionID': [],\n","                      'multi_uid': [],\n","                      }\n","\n","    for i in range(len(df)):\n","        item = df.iloc[i]\n","        if item['TransactionID'] not in problem_items['TransactionID']:\n","            mask_1 = full_df['card1'] == item['card1']\n","            mask_2 = full_df['uid_td_D1'] == item['uid_td_D1']\n","            mask_3 = full_df['TransactionID'] < item['TransactionID']\n","            mask_4 = ((full_df['DT_day'] == item['uid_td_D3'] + 1)|\n","                      (full_df['DT_day'] == item['uid_td_D3'] - 1)|\n","                      (full_df['DT_day'] == item['uid_td_D3']))\n","\n","            df_masked = full_df[mask_1 & mask_2 & mask_3 & mask_4]\n","            has_match = len(df_masked) > 0\n","\n","            if has_match:\n","                new_uids_items['TransactionID'].append(item['TransactionID'])\n","                new_uids_items['multi_uid'].append(list(df_masked['uid'].unique()))\n"," \n","    return_df = pd.DataFrame.from_dict(new_uids_items)\n","    return return_df\n","\n","def find_and_filter_groups(df):\n","    filtered_groups = []\n","\n","    for i in range(len(df)):\n","        test_id = df.iloc[i]['TransactionID']\n","        test_item = all_items[all_items['TransactionID']==test_id].iloc[0]\n","        possible_groups = df.iloc[i]['multi_uid']\n","        clean_group = find_right_uid(possible_groups, test_item)\n","        filtered_groups.append([test_id, clean_group])\n","    filtered_groups = pd.DataFrame(filtered_groups, columns=['TransactionID','uid'])    \n","    return filtered_groups\n","\n","import operator\n","\n","def find_right_uid(possible_groups, test_item):\n","    separated_uids = {}\n","\n","    test_features_set1 = {\n","        'TransactionAmt':2,\n","        'card2':1,\n","        'card3':1,\n","        'card4':1,\n","        'card5':1,\n","        'card6':1,\n","        'uid_td_D2':2,\n","        'uid_td_D10':2,\n","        'uid_td_D11':2,\n","        'uid_td_D15':2,\n","        'C14':1,\n","        'addr1':1,\n","        'addr2':1,\n","        'P_emaildomain':1,\n","        'V313_fix':1,\n","        }\n","\n","    groups_score = {}\n","\n","    for possible_group in possible_groups:\n","        masked_df = full_df[full_df['uid']==possible_group]\n","        cur_score = 0\n","        for col in test_features_set1:\n","            if test_item[col] in list(masked_df[col]):\n","                cur_score += test_features_set1[col]\n","        \n","        for col in v_cols:\n","            if test_item[col]!=0:\n","                if test_item[col+'_fix_ground'] in list(masked_df[col+'_fix']):\n","                    cur_score += 1\n","                    \n","        check_if_match = masked_df.append(test_item)\n","        check_if_match = sanity_check_run(check_if_match)\n","        if len(check_if_match)==0:\n","            groups_score[possible_group] = cur_score\n","        \n","    new_uid = np.nan\n","    try:\n","        new_uid = max(groups_score.items(), key=operator.itemgetter(1))[0]\n","    except:\n","        pass\n","    return new_uid"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Items to check: 86\n","Assigned items: 86\n","Inconsistent groups 0\n","Cleaning done...\n","Total groups: 4173 | Total items: 5704 | Total fraud 210\n"]}],"source":["########################### PART X - > With multigroup check\n","nan_df = all_items[~all_items['TransactionID'].isin(full_df['TransactionID'])]\n","print('Items to check:', len(nan_df))\n","\n","df_cleaned = df_parallelize_run(nan_df, find_multigroup)\n","df_cleaned = df_cleaned[~df_cleaned['multi_uid'].isna()]\n","\n","filtered_groups = df_parallelize_run(df_cleaned, find_and_filter_groups)\n","filtered_groups.index = filtered_groups['TransactionID']\n","temp_dict = filtered_groups['uid'].to_dict()\n","nan_df['uid'] = nan_df['TransactionID'].map(temp_dict)\n","nan_df = nan_df[~nan_df['uid'].isna()]\n","print('Assigned items:', len(nan_df))\n","\n","# append found items\n","full_df = pd.concat([full_df,nan_df]).sort_values(by='TransactionID').reset_index(drop=True)\n","check_state()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found bad items 13\n","Found bad items 2\n","Inconsistent groups 0\n","Cleaning done...\n","Total groups: 4173 | Total items: 5689 | Total fraud 210\n"]}],"source":["for i in range(100):\n","    bad_uids_groups = sanity_check_run(full_df, False)\n","    if len(bad_uids_groups)==0:\n","        break\n","    elif len(bad_uids_groups)>64:\n","        bad_uids_items = df_parallelize_run(bad_uids_groups, parallel_check)\n","    else:\n","        bad_uids_items = parallel_check(bad_uids_groups)\n","\n","    print('Found bad items', len(bad_uids_items))\n","    full_df['uid'] = np.where(full_df['TransactionID'].isin(bad_uids_items['uid']), np.nan, full_df['uid'])\n","    full_df = full_df[~full_df['uid'].isna()].sort_values(by='TransactionID').reset_index(drop=True)\n","    if len(bad_uids_items)<2:\n","        break\n","check_state()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Start items: 10000\n","Start items Frauds: 265\n"]}],"source":["print('Start items:', len(bkp_items))\n","print('Start items Frauds:', bkp_items['isFraud'].sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Uids Items: 9985\n","Uids Frauds: 265\n"]}],"source":["print('Uids Items:', len(single_items)+len(full_df))\n","print('Uids Frauds:', full_df['isFraud'].sum() + single_items['isFraud'].sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["full_df_final = pd.concat([full_df, \n","                     single_items, \n","                     global_bad_items,\n","                     all_items[~all_items['TransactionID'].isin(full_df['TransactionID'])]\n","                    ])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Combined Uids: 10000\n","CombinedUids Frauds: 265\n"]}],"source":["print('Combined Uids:', len(full_df_final))\n","print('CombinedUids Frauds:', full_df_final['isFraud'].sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["2.207945157321146"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["full_df['count'] = full_df.groupby(['uid'])['TransactionID'].transform('count')\n","full_df['count'].mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["8470"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["len(full_df_final['uid'].unique())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Inconsistent groups 0\n","Cleaning done...\n","Total groups: 4173 | Total items: 5689 | Total fraud 210\n"]}],"source":["check_state()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["----\n","### Export"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["########################### Export\n","full_df_final[['TransactionID','uid']].to_csv('uids_full_v6.csv')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":4}
